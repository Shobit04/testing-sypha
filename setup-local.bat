@echo off
echo ğŸš€ Setting up Sypha C Local AI for Windows...

REM Check if Ollama is already installed
where ollama >nul 2>nul
if %errorlevel% equ 0 (
    echo âœ… Ollama is already installed
) else (
    echo ğŸ“¥ Please install Ollama from https://ollama.ai/download
    echo ğŸ“¥ After installation, add Ollama to your PATH and restart this script
    pause
    exit /b 1
)

REM Start Ollama service
echo ğŸ”„ Starting Ollama service...
start /B ollama serve

REM Wait for Ollama to start
echo â³ Waiting for Ollama to start...
timeout /t 5 /nobreak >nul

REM Check if Ollama is running
curl -s http://localhost:11434/api/tags >nul 2>nul
if %errorlevel% equ 0 (
    echo âœ… Ollama service is running
) else (
    echo âŒ Failed to start Ollama service
    echo ğŸ’¡ Make sure Ollama is in your PATH
    pause
    exit /b 1
)

REM Download required models
echo ğŸ“¥ Downloading Whisper model...
ollama pull whisper

echo ğŸ“¥ Downloading Gemma 2B model...
ollama pull gemma2b

REM Verify models are installed
echo ğŸ” Verifying models...
ollama list

echo.
echo ğŸ‰ Setup complete! You can now run Sypha C:
echo    npm install
echo    npm start
echo.
echo ğŸ’¡ Make sure to keep Ollama running: ollama serve
echo ğŸ’¡ You can stop Ollama by closing the terminal or using Task Manager
pause
